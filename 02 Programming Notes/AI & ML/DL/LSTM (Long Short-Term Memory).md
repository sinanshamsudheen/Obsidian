![[Pasted image 20250604152054.png]]
![[Pasted image 20250604152107.png]]

inorder to remember long term memory, we introduce a new state
![[Pasted image 20250604152152.png]]

![[Pasted image 20250604152251.png]]

In LSTM
![[Pasted image 20250604152232.png]]

![[Pasted image 20250604152413.png]]

in LSTM
![[Pasted image 20250604152457.png]]
here it stored the meaningful words(samosa here) in it's long term memory.

![[Pasted image 20250604152618.png]]

# Forget gate
![[Pasted image 20250604155948.png]]

# Input gate
![[Pasted image 20250604155935.png]]
# Output Gate
![[Pasted image 20250604160049.png]]

# Bidirectional RNN
![[Pasted image 20250604163428.png]]